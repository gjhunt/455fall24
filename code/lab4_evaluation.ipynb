{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library('caret')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = runif(N,0,2*pi)\n",
    "f = function(x)(x-pi)^2/10+sin(x)\n",
    "y = f(x)\n",
    "e = rnorm(N,0,1/10)\n",
    "y = y + e\n",
    "\n",
    "options(repr.plot.width = 5, repr.plot.height = 5, repr.plot.res = 100)\n",
    "plot(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data.frame(x=x,y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flds = createFolds(1:nrow(df),k=2)\n",
    "flds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[flds[[1]],]\n",
    "train_df = df[flds[[2]],]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(test_df)\n",
    "dim(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build on the testing data\n",
    "knn_mod = knnreg(y~.,data=train_df,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = predict(knn_mod,train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_train = sqrt(mean((train_df$y-train_preds)^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = predict(knn_mod,test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_test = sqrt(mean((test_df$y-test_preds)^2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x,y)\n",
    "xe = data.frame(x=sort(runif(1000,0,2*pi)))\n",
    "lines(xe$x,f(xe$x),col='blue',lwd=5)\n",
    "lines(xe$x,predict(knn_mod,xe),col='red',lwd=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notice how the training RMSE is typically lower than the testing RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flds = createFolds(1:nrow(df),k=5)\n",
    "flds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths(flds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "test_df = df[flds[[i]],]\n",
    "train_df = df[unlist(flds[-i]),]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(test_df)\n",
    "dim(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_mod = knnreg(y~.,data=train_df,k=10)\n",
    "\n",
    "train_preds = predict(knn_mod,train_df)\n",
    "RMSE_train = sqrt(mean((train_df$y-train_preds)^2))\n",
    "\n",
    "test_preds = predict(knn_mod,test_df)\n",
    "RMSE_test = sqrt(mean((test_df$y-test_preds)^2))\n",
    "\n",
    "RMSE_train\n",
    "RMSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's put this in a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split_eval = function(train_idx,test_idx){\n",
    "    test_df = df[test_idx,]\n",
    "    train_df = df[train_idx,]\n",
    "    \n",
    "    knn_mod = knnreg(y~.,data=train_df,k=5)\n",
    "\n",
    "    train_preds = predict(knn_mod,train_df)\n",
    "    RMSE_train = sqrt(mean((train_df$y-train_preds)^2))\n",
    "\n",
    "    test_preds = predict(knn_mod,test_df)\n",
    "    RMSE_test = sqrt(mean((test_df$y-test_preds)^2))\n",
    "\n",
    "    return(data.frame(train=RMSE_train,\n",
    "        test=RMSE_test\n",
    "                ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = c()\n",
    "for(i in 1:10)\n",
    "    vec[i] = i^2\n",
    "\n",
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sapply(1:10,function(i)i^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lapply(1:10,function(i)i^2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flds = createFolds(1:nrow(df),k=10)\n",
    "rmses = lapply(1:length(flds),function(i){\n",
    "    tdf = tt_split_eval(train_idx = unlist(flds[-i]),test_idx = flds[[i]])\n",
    "    tdf$i = i \n",
    "    return(tdf)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmses[[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = Reduce('rbind',rmses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library('reshape2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mRMSE = reshape2::melt(RMSE,id.vars='i')\n",
    "mRMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library('ggplot2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=mRMSE,mapping=aes(x=i,y=value,color=variable))+\n",
    "    geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in total summary we can summarize the RMSEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median(RMSE$test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd(RMSE$test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test/train/validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use this to choose a value of $k$ for KNN? Use a train/validate/test 3-way split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "flds = createFolds(1:nrow(df),k=10)\n",
    "flds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "test_idx = flds[[i]]\n",
    "trainval_idx = unlist(flds[-i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[test_idx,]\n",
    "trainval_df = df[trainval_idx,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(test_df)\n",
    "dim(trainval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tv_flds = createFolds(1:nrow(trainval_df),k=10)\n",
    "tv_flds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "val_idx = tv_flds[[j]]\n",
    "train_idx = unlist(tv_flds[-j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = trainval_df[train_idx,]\n",
    "val_df = trainval_df[val_idx,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(train_df)\n",
    "dim(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split_eval_k = function(train_idx,val_idx,k=1){\n",
    "    train_df = trainval_df[train_idx,]\n",
    "    val_df = trainval_df[val_idx,]\n",
    "    \n",
    "    knn_mod = knnreg(y~.,data=train_df,k=k)\n",
    "\n",
    "    train_preds = predict(knn_mod,train_df)\n",
    "    RMSE_train = sqrt(mean((train_df$y-train_preds)^2))\n",
    "\n",
    "    val_preds = predict(knn_mod,val_df)\n",
    "    RMSE_val = sqrt(mean((val_df$y-val_preds)^2))\n",
    "\n",
    "    return(data.frame(train=RMSE_train,\n",
    "        val=RMSE_val\n",
    "                ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split_eval_k(train_idx,val_idx,k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt_split_eval_k(train_idx,val_idx,k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RMSE = lapply(1:75,function(k){\n",
    "    tdf = tt_split_eval_k(train_idx,val_idx,k=k)\n",
    "    tdf$k = k\n",
    "    return(tdf)\n",
    "})\n",
    "RMSE = Reduce('rbind',RMSE)\n",
    "head(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mRMSE = melt(RMSE,id.vars='k')\n",
    "head(mRMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=mRMSE,mapping=aes(x=k,y=value,color=variable))+\n",
    "    geom_point()+\n",
    "    scale_x_sqrt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "which.min(RMSE$val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_df = RMSE[which.min(RMSE$val),]\n",
    "min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_mod = knnreg(y~.,data=trainval_df,k=min_df$k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = predict(knn_mod,test_df)\n",
    "RMSE_val = sqrt(mean((test_df$y-test_preds)^2))\n",
    "RMSE_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nested x-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "can I do this in a cross validated way? yes use nested cross validation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outer loop = split into test and trainval datasets\n",
    "# inner loop = MBP, split into train/val and search over k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flds = createFolds(1:nrow(df),k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RMSE = rep(NA,length(flds))\n",
    "\n",
    "for(i in 1:length(flds)){\n",
    "    \n",
    "    # split testing from trainval\n",
    "    test_idx = flds[[i]]\n",
    "    trainval_idx = unlist(flds[-i])\n",
    "    test_df = df[test_idx,]\n",
    "    trainval_df = df[trainval_idx,]\n",
    "     \n",
    "    #MODEL BUILDING PROCESS\n",
    "    tv_flds = createFolds(1:nrow(trainval_df),k=10)\n",
    "    \n",
    "    K_seq = seq(1,75)\n",
    "    VAL_MTX = array(NA,c(length(tv_flds),length(K_seq)))\n",
    "\n",
    "    # normally wouldn't include these two lines\n",
    "    TRAIN_MTX = array(NA,c(length(tv_flds),length(K_seq)))\n",
    "    TEST_MTX = array(NA,c(length(tv_flds),length(K_seq)))\n",
    "    \n",
    "    for(j in 1:length(tv_flds)){\n",
    "        \n",
    "        val_idx = tv_flds[[j]]\n",
    "        train_idx = unlist(tv_flds[-j])\n",
    "        train_df = trainval_df[train_idx,]\n",
    "        val_df = trainval_df[val_idx,]\n",
    "\n",
    "        for(k in K_seq){\n",
    "            knn_mod = knnreg(y~.,data=train_df,k=k)\n",
    "            val_preds = predict(knn_mod,val_df)\n",
    "            VAL_MTX[j,k] = sqrt(mean((val_df$y-val_preds)^2))\n",
    "            \n",
    "            # normally wouldn't include these lines\n",
    "            train_preds = predict(knn_mod,train_df)\n",
    "            test_preds = predict(knn_mod,test_df)\n",
    "            TRAIN_MTX[j,k] = sqrt(mean((train_df$y-train_preds)^2))\n",
    "            TEST_MTX[j,k] = sqrt(mean((test_df$y-test_preds)^2))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    VAL_K = apply(VAL_MTX,2,mean)\n",
    "    K_hat = K_seq[which.min(VAL_K)]\n",
    "    \n",
    "    knn_mod = knnreg(y~.,data=trainval_df,k=K_hat)\n",
    "    \n",
    "    # eval on testing data\n",
    "    test_preds = predict(knn_mod,test_df)\n",
    "    TEST_RMSE[i] = sqrt(mean((test_df$y-test_preds)^2))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the output of a single run of the **outer** loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=melt(TRAIN_MTX),mapping=aes(x=Var2,y=value,color=as.factor(Var1)))+geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=melt(VAL_MTX),mapping=aes(x=Var2,y=value,color=as.factor(Var1)))+geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ggplot(data=melt(TEST_MTX),mapping=aes(x=Var2,y=value,color=as.factor(Var1)))+geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to viz easier, let's average over inner x-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library('tidyr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttv = data.frame(train=apply(TRAIN_MTX,2,mean),\n",
    "           test=apply(TEST_MTX,2,mean),\n",
    "           val=apply(VAL_MTX,2,mean),\n",
    "           k=K_seq)\n",
    "ttv = ttv%>%pivot_longer(cols=c(train,test,val))\n",
    "\n",
    "head(ttv)\n",
    "ggplot(data=ttv,mapping=aes(x=k,y=value,color=name))+geom_point()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(VAL_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_hat = K_seq[which.min(VAL_K)]\n",
    "K_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(TEST_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(VAL_K)\n",
    "abline(h=mean(TEST_RMSE),col='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What was the point of this nested x-validation, really? Its *really* `TEST_RMSE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean(TEST_RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we build the final model for prediction? Basically pull out the inner loop but now use **all** of the data! (Don't hold out a test set in any simple or fancy way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_flds = createFolds(1:nrow(df),k=10) # use all df\n",
    "    \n",
    "K_seq = seq(1,75)\n",
    "VAL_MTX = array(NA,c(length(tv_flds),length(K_seq)))\n",
    "\n",
    "for(j in 1:length(tv_flds)){\n",
    "\n",
    "    val_idx = tv_flds[[j]]\n",
    "    train_idx = unlist(tv_flds[-j])\n",
    "    train_df = df[train_idx,]\n",
    "    val_df = df[val_idx,]\n",
    "\n",
    "    for(k in K_seq){\n",
    "        knn_mod = knnreg(y~.,data=train_df,k=k)\n",
    "        val_preds = predict(knn_mod,val_df)\n",
    "        VAL_MTX[j,k] = sqrt(mean((val_df$y-val_preds)^2))\n",
    "    }\n",
    "}\n",
    "\n",
    "VAL_K = apply(VAL_MTX,2,mean)\n",
    "K_hat = K_seq[which.min(VAL_K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(VAL_K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit with all the data\n",
    "knn_mod = knnreg(y~.,data=df,k=K_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boss: Ok, cool, how good will this do? `TEST_RMSE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x,y)\n",
    "lines(xe$x,f(xe$x),col='blue',lwd=5)\n",
    "lines(xe$x,predict(knn_mod,xe),col='red',lwd=5)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,Rmd,R"
  },
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
